{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is needed to ensure my working directory is the same as where the dataset is\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\wyman\\\\Desktop\\\\701IntroProject\\\\INM701')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is 1,000,000 rows but a sample size of 10000 is used to save time. However, it is stratified to maintain the same fraud ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE, trustworthiness\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "df = pd.read_csv('data/card_transdata.csv')\n",
    "\n",
    "# Get counts of fraud and non-fraud cases\n",
    "fraud_count = df['fraud'].value_counts()\n",
    "min_count = fraud_count.min()\n",
    "\n",
    "# Separate fraud and non-fraud cases\n",
    "fraud_df = df[df['fraud'] == 1].sample(n=5000, random_state=42)\n",
    "non_fraud_df = df[df['fraud'] == 0].sample(n=5000, random_state=42)\n",
    "\n",
    "# Combine the balanced datasets\n",
    "df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle the final dataframe\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block normalizes the numeric features using StandardScaler to ensure that the GMM algorithm performs effectively. StandardScaler normalizes the data such that the Mean equals 0 and the Standard Deviation equals 1. The binary columns are left unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Counts for Target Labels:\n",
      "fraud\n",
      "0.0    5000\n",
      "1.0    5000\n",
      "Name: count, dtype: int64\n",
      "Fraud ratio: 0.5\n"
     ]
    }
   ],
   "source": [
    "# separate features and labels\n",
    "X = df.drop(columns=['fraud'], axis=1) \n",
    "y = df['fraud'] \n",
    "\n",
    "# separate binary columns\n",
    "binary_columns = [col for col in X.columns if set(X[col].unique()) <= {0, 1}]\n",
    "\n",
    "# separate continuous columns\n",
    "continuous_columns = [col for col in X.columns if col not in binary_columns]\n",
    "\n",
    "# scale only the continuous columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled_continuous = scaler.fit_transform(df[continuous_columns])\n",
    "\n",
    "# combine binary and scaled continuous data\n",
    "X_scaled = pd.concat(\n",
    "    [pd.DataFrame(X_scaled_continuous, columns=continuous_columns), X[binary_columns].reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check that the classification labels were split properly\n",
    "print(\"\\nValue Counts for Target Labels:\")\n",
    "print(y.value_counts())\n",
    "original_fraud_ratio = y.mean()\n",
    "print(f\"Fraud ratio: {original_fraud_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, variance_threshold): # apply PCA dimension reduction to a given dataset\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca\n",
    "\n",
    "def apply_tsne(X, perplexity, learning_rate=200): # apply t-SNE dimension reduction to a given dataset\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, learning_rate=learning_rate)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    return X_tsne, tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation function for GMM\n",
    "def run_gmm_cv(X, y, n_components, covariance_type, cv=5):\n",
    "    # Convert to pandas objects if necessary\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    results = []\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        # Train GMM\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
    "        gmm.fit(X_train)\n",
    "        val_clusters = gmm.predict(X_val)\n",
    "        \n",
    "        # Map clusters to labels\n",
    "        mapped_clusters = map_clusters_to_labels(val_clusters, y_val, original_fraud_ratio)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_val, mapped_clusters)\n",
    "        precision = precision_score(y_val, mapped_clusters, pos_label=1, zero_division=0)\n",
    "        recall = recall_score(y_val, mapped_clusters, pos_label=1, zero_division=0)\n",
    "        f1 = f1_score(y_val, mapped_clusters, pos_label=1, zero_division=0)\n",
    "        \n",
    "        results.append({'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "    \n",
    "    # Calculate mean scores across folds\n",
    "    mean_results = {metric: np.mean([res[metric] for res in results]) for metric in results[0].keys()}\n",
    "    return mean_results\n",
    "\n",
    "\n",
    "\n",
    "def grid_search_gmm(X, y, n_components_range, covariance_types, method_name=\"\", verbose=True):\n",
    "    all_results = []\n",
    "\n",
    "    for n_components in n_components_range:\n",
    "        for cov_type in covariance_types:\n",
    "            # Run GMM cross-validation\n",
    "            score_dict = run_gmm_cv(X, y, n_components, cov_type, cv=5)\n",
    "            result = {\n",
    "                'Method': method_name,\n",
    "                'n_components': n_components,\n",
    "                'covariance_type': cov_type,\n",
    "                'Accuracy': score_dict['accuracy'],\n",
    "                'Precision': score_dict['precision'],\n",
    "                'Recall': score_dict['recall'],\n",
    "                'F1-Score': score_dict['f1']\n",
    "            }\n",
    "            all_results.append(result)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{method_name} - n_components: {n_components}, covariance_type: {cov_type}, \"\n",
    "                      f\"Accuracy: {score_dict['accuracy']:.4f}, F1: {score_dict['f1']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# assigns a cluster to fraud  if the proportion of fraud cases within it \n",
    "# exceeds the dataset's overall fraud ratio \n",
    "def map_clusters_to_labels(clusters, y_val, original_fraud_ratio):\n",
    "    results_df = pd.DataFrame({'Cluster': clusters, 'Actual': y_val})\n",
    "    cluster_label_mapping = pd.crosstab(results_df['Cluster'], results_df['Actual'])\n",
    "    cluster_to_label = {}\n",
    "    for cluster in cluster_label_mapping.index:\n",
    "        fraud_ratio_in_cluster = cluster_label_mapping.loc[cluster, 1.0] / cluster_label_mapping.loc[cluster].sum()\n",
    "        cluster_to_label[cluster] = 1.0 if fraud_ratio_in_cluster > original_fraud_ratio else 0.0\n",
    "    mapped_labels = [cluster_to_label[c] for c in clusters]\n",
    "    return mapped_labels\n",
    "\n",
    "\n",
    "def run_dimensionality_reduction_and_cv(X, y, method=None, n_splits=5,\n",
    "                                        n_clusters=None, covariance_type='tied',\n",
    "                                        pca_threshold=None, tsne_perplexity=None):    \n",
    "    if method == \"PCA\":\n",
    "        print(\"Applying PCA\")\n",
    "        X_reduced, pca_obj = apply_pca(X, pca_threshold)\n",
    "    elif method == \"TSNE\":\n",
    "        print(\"Applying t-SNE\")\n",
    "        X_reduced, tsne_obj = apply_tsne(X, tsne_perplexity)\n",
    "    elif method is None:\n",
    "        print(\"No dimensionality reduction\")\n",
    "        X_reduced = X\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    print(f\"\\nRunning Cross-Validation ({'No Dimensionality Reduction' if method is None else method})...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_reduced, y)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        X_train, X_val = X_reduced.iloc[train_idx], X_reduced.iloc[val_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        # Train GMM for each fold\n",
    "        gmm = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, random_state=42)\n",
    "        gmm.fit(X_train)\n",
    "        val_clusters = gmm.predict(X_val)\n",
    "        mapped_val_clusters = map_clusters_to_labels(val_clusters, y_val, original_fraud_ratio)\n",
    "\n",
    "        # Calculate performance metrics for each fold\n",
    "        accuracy = accuracy_score(y_val, mapped_val_clusters)\n",
    "        precision = precision_score(y_val, mapped_val_clusters, pos_label=1, zero_division=0)\n",
    "        recall = recall_score(y_val, mapped_val_clusters, pos_label=1, zero_division=0)\n",
    "        f1 = f1_score(y_val, mapped_val_clusters, pos_label=1, zero_division=0)\n",
    "        fold_results.append((accuracy, precision, recall, f1))\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    avg_metrics = np.mean(fold_results, axis=0)\n",
    "    print(f\"\\nAverage Metrics Across Folds: Accuracy: {avg_metrics[0]:.4f}, Precision: {avg_metrics[1]:.4f}, \"\n",
    "          f\"Recall: {avg_metrics[2]:.4f}, F1-Score: {avg_metrics[3]:.4f}\")\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# evaluate each testing function based on a different dimensional reduction using the optimal parameters\n",
    "def evaluate_on_test_set(X_train, X_test, y_test, method=None,\n",
    "                         best_no_reduction_result=None,\n",
    "                         best_pca_result=None, best_tsne_result=None,\n",
    "                         pca_threshold=None, tsne_perplexity=None):    \n",
    "    if method == \"PCA\":\n",
    "        # Apply PCA and retrieve optimal parameters\n",
    "        X_train, pca_obj = apply_pca(X_train, pca_threshold)\n",
    "        X_test = pca_obj.transform(X_test)\n",
    "        n_clusters = int(best_pca_result['n_components'])\n",
    "        covariance_type = best_pca_result['covariance_type']\n",
    "    elif method == \"t-SNE\":\n",
    "        # Apply t-SNE and retrieve optimal parameters\n",
    "        X_train, _ = apply_tsne(X_train, tsne_perplexity)\n",
    "        X_test, _ = apply_tsne(X_test, tsne_perplexity)\n",
    "        n_clusters = int(best_tsne_result['n_components'])\n",
    "        covariance_type = best_tsne_result['covariance_type']\n",
    "    elif method is None:\n",
    "        # Use the best parameters for no reduction\n",
    "        n_clusters = int(best_no_reduction_result['n_components'])\n",
    "        covariance_type = best_no_reduction_result['covariance_type']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dimensionality reduction method\")\n",
    "\n",
    "    # Fit GMM with the optimal parameters\n",
    "    gmm = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, random_state=42)\n",
    "    gmm.fit(X_train)\n",
    "    test_clusters = gmm.predict(X_test)\n",
    "\n",
    "    # Map clusters to labels\n",
    "    mapped_test_clusters = map_clusters_to_labels(test_clusters, y_test, original_fraud_ratio)\n",
    "    cm = confusion_matrix(y_test, mapped_test_clusters)\n",
    "\n",
    "    return cm, X_test, test_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_performance(x_values, y_values, title, xlabel, ylabel, labels=None):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    if labels is None:\n",
    "        labels = [None]*len(y_values)\n",
    "    for y, lbl in zip(y_values, labels):\n",
    "        plt.plot(x_values, y, marker='o', label=lbl)\n",
    "    if any(labels):\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_conf_matrix(cm, class_names=[\"Non-Fraud\",\"Fraud\"], title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Reduction Optimization\n",
    "\n",
    "A grid search is performed to find the optimal set of hyperparameters for the model assuming the dataset has not gone through any dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing GMM grid search with no dimensionality reduction...\n",
      "n_components: 2, covariance_type: spherical, Silhouette_Score: 0.5509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyman\\AppData\\Local\\Temp\\ipykernel_3248\\2257159952.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  no_reduction_gmm_results = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components: 2, covariance_type: diag, Silhouette_Score: 0.2361\n",
      "n_components: 2, covariance_type: tied, Silhouette_Score: 0.6815\n",
      "n_components: 2, covariance_type: full, Silhouette_Score: 0.2366\n",
      "n_components: 3, covariance_type: spherical, Silhouette_Score: 0.1964\n",
      "n_components: 3, covariance_type: diag, Silhouette_Score: 0.1791\n",
      "n_components: 3, covariance_type: tied, Silhouette_Score: 0.4629\n",
      "n_components: 3, covariance_type: full, Silhouette_Score: 0.1802\n",
      "n_components: 4, covariance_type: spherical, Silhouette_Score: 0.1347\n",
      "n_components: 4, covariance_type: diag, Silhouette_Score: 0.1558\n",
      "n_components: 4, covariance_type: tied, Silhouette_Score: 0.4778\n",
      "n_components: 4, covariance_type: full, Silhouette_Score: 0.1608\n",
      "n_components: 5, covariance_type: spherical, Silhouette_Score: 0.1642\n",
      "n_components: 5, covariance_type: diag, Silhouette_Score: 0.0813\n",
      "n_components: 5, covariance_type: tied, Silhouette_Score: 0.4946\n",
      "n_components: 5, covariance_type: full, Silhouette_Score: 0.0844\n",
      "\n",
      "Best No Reduction GMM Result:\n",
      "n_components               2\n",
      "covariance_type         tied\n",
      "Silhouette_Score    0.681515\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Reset indices for X_scaled and y to ensure alignment\n",
    "if isinstance(X_scaled, pd.DataFrame):\n",
    "    X_scaled = X_scaled.reset_index(drop=True)\n",
    "else:\n",
    "    X_scaled = pd.DataFrame(X_scaled)\n",
    "\n",
    "y = pd.Series(y).reset_index(drop=True)\n",
    "\n",
    "# Define GMM hyperparameters\n",
    "n_components_range = range(2, 6)  # Number of clusters for GMM\n",
    "covariance_types = ['spherical', 'diag', 'tied', 'full']  # GMM covariance types\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "no_reduction_gmm_results = pd.DataFrame(columns=['n_components', 'covariance_type', 'Silhouette_Score'])\n",
    "\n",
    "# Perform grid search for GMM on the original data (no reduction)\n",
    "print(\"\\nPerforming GMM grid search with no dimensionality reduction...\")\n",
    "\n",
    "# Iterate over all combinations of n_components and covariance_type\n",
    "for n_clusters in n_components_range:\n",
    "    for covariance_type in covariance_types:\n",
    "        # Train GMM to compute Silhouette Score\n",
    "        gmm = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, random_state=42)\n",
    "        labels = gmm.fit_predict(X_scaled)\n",
    "        silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "\n",
    "        # Print the result for the current combination\n",
    "        print(f\"n_components: {n_clusters}, covariance_type: {covariance_type}, Silhouette_Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "        # Append results to DataFrame\n",
    "        no_reduction_gmm_results = pd.concat(\n",
    "            [no_reduction_gmm_results, pd.DataFrame({\n",
    "                'n_components': [n_clusters],\n",
    "                'covariance_type': [covariance_type],\n",
    "                'Silhouette_Score': [silhouette_avg]\n",
    "            })],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "# Find the best combination of n_components and covariance_type based on Silhouette Score\n",
    "best_no_reduction_result = no_reduction_gmm_results.loc[no_reduction_gmm_results['Silhouette_Score'].idxmax()]\n",
    "\n",
    "# Print the best result\n",
    "print(\"\\nBest No Reduction GMM Result:\")\n",
    "print(best_no_reduction_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for No Reduction GMM Results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Define unique markers for each covariance type\n",
    "covariance_markers = {\n",
    "    'spherical': 'o',  # circle\n",
    "    'diag': 's',       # square\n",
    "    'tied': '^',       # triangle\n",
    "    'full': 'D'        # diamond\n",
    "}\n",
    "\n",
    "# Assign unique colors to each covariance type\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(covariance_types)))\n",
    "covariance_color_map = dict(zip(covariance_types, colors))\n",
    "\n",
    "# Loop through each covariance type\n",
    "for covariance_type in covariance_types:\n",
    "    # Filter results for the current covariance type\n",
    "    filtered_df = no_reduction_gmm_results[no_reduction_gmm_results['covariance_type'] == covariance_type]\n",
    "\n",
    "    # Extract number of components and silhouette scores\n",
    "    n_components = filtered_df['n_components'].values\n",
    "    scores = filtered_df['Silhouette_Score'].values\n",
    "\n",
    "    # Plot the scores\n",
    "    plt.plot(\n",
    "        n_components, scores,\n",
    "        marker=covariance_markers[covariance_type],  # Unique marker for covariance type\n",
    "        linestyle='-', \n",
    "        color=covariance_color_map[covariance_type],  # Color for each covariance type\n",
    "        label=f'Covariance Type: {covariance_type}'\n",
    "    )\n",
    "\n",
    "# Finalize the plot\n",
    "plt.title(\"Silhouette Scores for No Reduction GMM Clusters and Covariance Types\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.legend(title=\"Covariance Type\", fontsize=8, loc='best')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is run with various combinations of retained variance thresholds, covariance types and cluster amount combinations to determine the optimal set of parameters based primarily on the silhouette score which is a value that measures how well connected the clusters are (geometric seperation). The best combination of parameters is maintained for later testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 0.7 - n_components: 2, covariance_type: spherical, Silhouette_Score: 0.5285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyman\\AppData\\Local\\Temp\\ipykernel_3248\\2010711674.py:35: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pca_gmm_results = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 0.7 - n_components: 2, covariance_type: diag, Silhouette_Score: 0.5125\n",
      "PCA 0.7 - n_components: 2, covariance_type: tied, Silhouette_Score: 0.6793\n",
      "PCA 0.7 - n_components: 2, covariance_type: full, Silhouette_Score: 0.3490\n",
      "PCA 0.7 - n_components: 3, covariance_type: spherical, Silhouette_Score: 0.3169\n",
      "PCA 0.7 - n_components: 3, covariance_type: diag, Silhouette_Score: 0.2362\n",
      "PCA 0.7 - n_components: 3, covariance_type: tied, Silhouette_Score: 0.6925\n",
      "PCA 0.7 - n_components: 3, covariance_type: full, Silhouette_Score: 0.2243\n",
      "PCA 0.7 - n_components: 4, covariance_type: spherical, Silhouette_Score: 0.3994\n",
      "PCA 0.7 - n_components: 4, covariance_type: diag, Silhouette_Score: 0.2692\n",
      "PCA 0.7 - n_components: 4, covariance_type: tied, Silhouette_Score: 0.7051\n",
      "PCA 0.7 - n_components: 4, covariance_type: full, Silhouette_Score: 0.1985\n",
      "PCA 0.7 - n_components: 5, covariance_type: spherical, Silhouette_Score: 0.3805\n",
      "PCA 0.7 - n_components: 5, covariance_type: diag, Silhouette_Score: 0.3105\n",
      "PCA 0.7 - n_components: 5, covariance_type: tied, Silhouette_Score: 0.5208\n",
      "PCA 0.7 - n_components: 5, covariance_type: full, Silhouette_Score: 0.1141\n",
      "PCA 0.8 - n_components: 2, covariance_type: spherical, Silhouette_Score: 0.5285\n",
      "PCA 0.8 - n_components: 2, covariance_type: diag, Silhouette_Score: 0.5125\n",
      "PCA 0.8 - n_components: 2, covariance_type: tied, Silhouette_Score: 0.6793\n",
      "PCA 0.8 - n_components: 2, covariance_type: full, Silhouette_Score: 0.3490\n",
      "PCA 0.8 - n_components: 3, covariance_type: spherical, Silhouette_Score: 0.3169\n",
      "PCA 0.8 - n_components: 3, covariance_type: diag, Silhouette_Score: 0.2362\n",
      "PCA 0.8 - n_components: 3, covariance_type: tied, Silhouette_Score: 0.6925\n",
      "PCA 0.8 - n_components: 3, covariance_type: full, Silhouette_Score: 0.2243\n",
      "PCA 0.8 - n_components: 4, covariance_type: spherical, Silhouette_Score: 0.3994\n",
      "PCA 0.8 - n_components: 4, covariance_type: diag, Silhouette_Score: 0.2692\n",
      "PCA 0.8 - n_components: 4, covariance_type: tied, Silhouette_Score: 0.7051\n",
      "PCA 0.8 - n_components: 4, covariance_type: full, Silhouette_Score: 0.1985\n",
      "PCA 0.8 - n_components: 5, covariance_type: spherical, Silhouette_Score: 0.3805\n",
      "PCA 0.8 - n_components: 5, covariance_type: diag, Silhouette_Score: 0.3105\n",
      "PCA 0.8 - n_components: 5, covariance_type: tied, Silhouette_Score: 0.5208\n",
      "PCA 0.8 - n_components: 5, covariance_type: full, Silhouette_Score: 0.1141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m removed_components \u001b[38;5;241m=\u001b[39m original_components \u001b[38;5;241m-\u001b[39m retained_components\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# perform grid search for GMM on the PCA-reduced data\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m gmm_results_df \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_gmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPCA-GMM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# calculate silhouette scores for each combination\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m gmm_results_df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "Cell \u001b[1;32mIn[86], line 44\u001b[0m, in \u001b[0;36mgrid_search_gmm\u001b[1;34m(X, y, n_components_range, covariance_types, method_name, verbose)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_components \u001b[38;5;129;01min\u001b[39;00m n_components_range:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cov_type \u001b[38;5;129;01min\u001b[39;00m covariance_types:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# Run GMM cross-validation\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m         score_dict \u001b[38;5;241m=\u001b[39m \u001b[43mrun_gmm_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m         result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m'\u001b[39m: method_name,\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_components\u001b[39m\u001b[38;5;124m'\u001b[39m: n_components,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-Score\u001b[39m\u001b[38;5;124m'\u001b[39m: score_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     53\u001b[0m         }\n\u001b[0;32m     54\u001b[0m         all_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[86], line 18\u001b[0m, in \u001b[0;36mrun_gmm_cv\u001b[1;34m(X, y, n_components, covariance_type, cv)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train GMM\u001b[39;00m\n\u001b[0;32m     17\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39mn_components, covariance_type\u001b[38;5;241m=\u001b[39mcovariance_type, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m val_clusters \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Map clusters to labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\mixture\\_base.py:181\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\mixture\\_base.py:248\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    245\u001b[0m prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[0;32m    247\u001b[0m log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(X)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n\u001b[0;32m    251\u001b[0m change \u001b[38;5;241m=\u001b[39m lower_bound \u001b[38;5;241m-\u001b[39m prev_lower_bound\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:813\u001b[0m, in \u001b[0;36mGaussianMixture._m_step\u001b[1;34m(self, X, log_resp)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, log_resp):\n\u001b[0;32m    802\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"M step.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_ \u001b[38;5;241m=\u001b[39m _estimate_gaussian_parameters(\n\u001b[1;32m--> 813\u001b[0m         X, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_covar, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[0;32m    814\u001b[0m     )\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecisions_cholesky_ \u001b[38;5;241m=\u001b[39m _compute_precision_cholesky(\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[0;32m    818\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define PCA thresholds and RF hyperparameters\n",
    "pca_thresholds = [0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "n_components_range = range(2, 6)  # Number of clusters for GMM\n",
    "covariance_types = ['spherical', 'diag', 'tied', 'full']  # GMM covariance types\n",
    "\n",
    "# store results \n",
    "pca_gmm_results = pd.DataFrame(columns=['pca_threshold', 'n_components', 'covariance_type', 'Silhouette_Score'])\n",
    "\n",
    "original_components = X_scaled.shape[1]\n",
    "\n",
    "# loop through PCA thresholds and hyperparameter combinations\n",
    "for pca_threshold in pca_thresholds:\n",
    "    # apply PCA\n",
    "    X_pca, pca_obj = apply_pca(X_scaled, pca_threshold)\n",
    "    retained_components = pca_obj.n_components_\n",
    "    removed_components = original_components - retained_components\n",
    "\n",
    "    # perform grid search for GMM on the PCA-reduced data\n",
    "    gmm_results_df = grid_search_gmm(X_pca, y, n_components_range, covariance_types, method_name=\"PCA-GMM\", verbose=False)\n",
    "\n",
    "    # calculate silhouette scores for each combination\n",
    "    for _, row in gmm_results_df.iterrows():\n",
    "        n_clusters = row['n_components']\n",
    "        covariance_type = row['covariance_type']\n",
    "\n",
    "        # train GMM to compute Silhouette Score\n",
    "        gmm = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, random_state=42)\n",
    "        labels = gmm.fit_predict(X_pca)\n",
    "        silhouette_avg = silhouette_score(X_pca, labels)\n",
    "\n",
    "        # Print the result for the current combination\n",
    "        print(f\"PCA {pca_threshold} - n_components: {n_clusters}, covariance_type: {covariance_type}, Silhouette_Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "        # append results to DataFrame\n",
    "        pca_gmm_results = pd.concat(\n",
    "            [pca_gmm_results, pd.DataFrame({\n",
    "                'pca_threshold': [pca_threshold],\n",
    "                'n_components': [n_clusters],\n",
    "                'covariance_type': [covariance_type],\n",
    "                'Silhouette_Score': [silhouette_avg]\n",
    "            })],\n",
    "            ignore_index=True\n",
    "        )\n",
    "# find the best PCA threshold, n_clusters, and covariance_type based on Silhouette Score\n",
    "best_pca_result = pca_gmm_results.loc[pca_gmm_results['Silhouette_Score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest PCA-GMM Result:\")\n",
    "print(best_pca_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# define unique markers for each covariance type\n",
    "covariance_markers = {\n",
    "    'spherical': 'o',  # circle\n",
    "    'diag': 's',       # square\n",
    "    'tied': '^',       # triangle\n",
    "    'full': 'D'        # diamond\n",
    "}\n",
    "\n",
    "# use consistent colors for each PCA threshold\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(pca_thresholds)))\n",
    "threshold_color_map = dict(zip(pca_thresholds, colors))\n",
    "\n",
    "# Loop through each PCA threshold and covariance type\n",
    "for threshold in pca_thresholds:\n",
    "    for covariance_type in covariance_types:\n",
    "        # filter the DataFrame for the current PCA threshold and covariance type\n",
    "        filtered_df = pca_gmm_results[\n",
    "            (pca_gmm_results['pca_threshold'] == threshold) &\n",
    "            (pca_gmm_results['covariance_type'] == covariance_type)\n",
    "        ]\n",
    "        \n",
    "        # extract number of components and silhouette scores\n",
    "        n_components = filtered_df['n_components'].values\n",
    "        scores = filtered_df['Silhouette_Score'].values\n",
    "\n",
    "        # plot the scores with specific color and marker\n",
    "        plt.plot(\n",
    "            n_components, scores,\n",
    "            marker=covariance_markers[covariance_type],  # unique marker for covariance type\n",
    "            linestyle='-', \n",
    "            color=threshold_color_map[threshold],  # same color for each PCA threshold\n",
    "            label=f'PCA {threshold} - {covariance_type}'\n",
    "        )\n",
    "\n",
    "# Finalize the plot\n",
    "plt.title(\"Silhouette Scores for PCA Thresholds, GMM Clusters, and Covariance Types\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.legend(title=\"PCA Threshold and Covariance Type\", fontsize=8, loc='best')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are a PCA threshold of 0.9, 2 clusters, and a tied covariance type. The silhouette score decreases as the number of clusters increases, which makes sense since the data has 2 classes (fraud and non-fraud), so each class naturally forms its own cluster. The tied covariance performs best because the fraud and non fraud clusters have similar shapes, and the tied covariance type is less prone to overfitting compared to other covariance types because it assumes all clusters share the same covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is run with a range of perplexities, covariance types, and cluster amount combinations to determine the optimal set of parameters based primarily on the silhouette score and the Adjusted Rand Index (ARI). The trustworthiness is useful to verify that the local structure of the data is properly reflected after the dimension reduction, but the other metrics better measure how well the data is preservered after being reduced. ARI is used for t-SNE and not PCA because ARI evaluates cluster alignment with true labels and t-SNE tries to preserve local similarities in data, unlike PCA which has to preserve linearity. t-SNE doesn't always prefer 2 clusters. I think this is because this method always reduces the data to 2 dimensions (compared to PCA which is variable) which can substantially alter the way the data is preserved, causing the model to recognize more clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define t-SNE perplexities and GMM hyperparameters\n",
    "tsne_perplexities = [5, 10, 20, 30, 40]\n",
    "n_components_range = range(2, 6)  # Number of clusters for GMM\n",
    "covariance_types = ['spherical', 'diag', 'tied', 'full']  # GMM covariance types\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "tsne_gmm_results = pd.DataFrame(columns=[\n",
    "    'tsne_perplexity', 'n_components', 'covariance_type', 'Silhouette_Score', 'ARI', 'Trustworthiness'\n",
    "])\n",
    "\n",
    "# Loop through t-SNE perplexities\n",
    "for perplexity in tsne_perplexities:\n",
    "    # Apply t-SNE\n",
    "    print(f\"\\nApplying t-SNE with Perplexity: {perplexity}\")\n",
    "    X_tsne, tsne_obj = apply_tsne(X_scaled, perplexity, learning_rate=200)\n",
    "\n",
    "    # Calculate trustworthiness for the reduced data\n",
    "    trust = trustworthiness(X_scaled, X_tsne, n_neighbors=5)\n",
    "    print(f\"Trustworthiness: {trust:.4f}\")\n",
    "\n",
    "    # Perform GMM grid search on the t-SNE-reduced data\n",
    "    gmm_results_df = grid_search_gmm(X_tsne, y, n_components_range, covariance_types, method_name=f\"t-SNE-{perplexity}\", verbose=False)\n",
    "\n",
    "    # Calculate silhouette and ARI scores for each GMM configuration\n",
    "    for _, row in gmm_results_df.iterrows():\n",
    "        n_clusters = row['n_components']\n",
    "        covariance_type = row['covariance_type']\n",
    "\n",
    "        # Fit GMM and compute metrics\n",
    "        gmm = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, random_state=42)\n",
    "        labels = gmm.fit_predict(X_tsne)\n",
    "        silhouette_avg = silhouette_score(X_tsne, labels)\n",
    "        ari_score = adjusted_rand_score(y, labels)\n",
    "\n",
    "        # Print the result for the current combination\n",
    "        print(f\"t-SNE {perplexity} - n_components: {n_clusters}, covariance_type: {covariance_type}, \"\n",
    "              f\"Silhouette_Score: {silhouette_avg:.4f}, ARI: {ari_score:.4f}\")\n",
    "\n",
    "        # Append results to DataFrame\n",
    "        tsne_gmm_results = pd.concat([\n",
    "            tsne_gmm_results,\n",
    "            pd.DataFrame([{\n",
    "                'tsne_perplexity': perplexity,\n",
    "                'n_components': n_clusters,\n",
    "                'covariance_type': covariance_type,\n",
    "                'Silhouette_Score': silhouette_avg,\n",
    "                'ARI': ari_score,\n",
    "                'Trustworthiness': trust\n",
    "            }])\n",
    "        ], ignore_index=True)\n",
    "\n",
    "# Find the best t-SNE configuration based on ARI and silhouette score\n",
    "best_tsne_result = tsne_gmm_results.sort_values(['ARI', 'Silhouette_Score'], ascending=[False, False]).iloc[0]\n",
    "print(f\"\\nBest t-SNE Result:\\n{best_tsne_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Define unique markers for covariance types\n",
    "covariance_markers = {\n",
    "    'spherical': 'o',\n",
    "    'diag': 's',\n",
    "    'tied': '^',\n",
    "    'full': 'D'\n",
    "}\n",
    "\n",
    "# Use consistent colors for each t-SNE perplexity\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(tsne_perplexities)))\n",
    "perplexity_color_map = dict(zip(tsne_perplexities, colors))\n",
    "\n",
    "# Loop through each t-SNE perplexity and covariance type\n",
    "for perplexity in tsne_perplexities:\n",
    "    for covariance_type in covariance_types:\n",
    "        # Filter DataFrame for the current perplexity and covariance type\n",
    "        filtered_df = tsne_gmm_results[\n",
    "            (tsne_gmm_results['tsne_perplexity'] == perplexity) &\n",
    "            (tsne_gmm_results['covariance_type'] == covariance_type)\n",
    "        ]\n",
    "\n",
    "        # Extract clusters and silhouette scores\n",
    "        n_components = filtered_df['n_components'].values\n",
    "        scores = filtered_df['Silhouette_Score'].values\n",
    "\n",
    "        # Plot the scores\n",
    "        plt.plot(\n",
    "            n_components, scores,\n",
    "            marker=covariance_markers[covariance_type],\n",
    "            linestyle='-', color=perplexity_color_map[perplexity],\n",
    "            label=f't-SNE {perplexity} - {covariance_type}'\n",
    "        )\n",
    "\n",
    "# Finalize the Silhouette Score plot\n",
    "plt.title(\"Silhouette Scores for t-SNE Perplexities, Covariance Types, and GMM Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.legend(title=\"Perplexity and Covariance Type\", fontsize=8, loc='best')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a correlation between the different parameters for the silhouette score, besides the performance steadily decreasing as the amount of clusters increase. The ARI score significantly decreases as the number of clusters increase. Additionally, the ARI score goes down as the perplexity increases, likely because higher perplexity values causes clusters to spread out and overlap more which can reduce their alignment with the true class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Trained Parameters\n",
    "This block aggregates the best hyperparameters for each dataset and prints them to summarize information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and store the best hyperparameters for No Reduction\n",
    "print(\"\\nUsing the best hyperparameters for No Reduction\")\n",
    "best_no_reduction_n_components = best_no_reduction_result['n_components']\n",
    "best_no_reduction_covariance_type = best_no_reduction_result['covariance_type']\n",
    "\n",
    "# Extract and store the best hyperparameters for PCA\n",
    "print(\"\\nUsing the best hyperparameters for PCA\")\n",
    "best_pca_threshold = best_pca_result['pca_threshold']\n",
    "best_pca_n_components = best_pca_result['n_components']\n",
    "best_pca_covariance_type = best_pca_result['covariance_type']\n",
    "\n",
    "# Extract and store the best hyperparameters for t-SNE\n",
    "print(\"\\nUsing the best hyperparameters for t-SNE\")\n",
    "best_tsne_perplexity = best_tsne_result['tsne_perplexity']\n",
    "best_tsne_n_components = best_tsne_result['n_components']\n",
    "best_tsne_covariance_type = best_tsne_result['covariance_type']\n",
    "\n",
    "# Print the recap of the best hyperparameters\n",
    "print(\"\\nBest GMM results for No Reduction:\")\n",
    "print(f\"Best n_components: {best_no_reduction_n_components}\")\n",
    "print(f\"Best covariance_type: {best_no_reduction_covariance_type}\")\n",
    "\n",
    "print(\"\\nBest GMM results for PCA:\")\n",
    "print(f\"Best PCA Threshold: {best_pca_threshold}\")\n",
    "print(f\"Best n_components: {best_pca_n_components}\")\n",
    "print(f\"Best covariance_type: {best_pca_covariance_type}\")\n",
    "\n",
    "print(\"\\nBest GMM results for t-SNE:\")\n",
    "print(f\"Best t-SNE Perplexity: {best_tsne_perplexity}\")\n",
    "print(f\"Best n_components: {best_tsne_n_components}\")\n",
    "print(f\"Best covariance_type: {best_tsne_covariance_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Metrics\n",
    "\n",
    "This block runs cross validation to test the performance metrics with each form of dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter configurations for each method\n",
    "method_configs = {\n",
    "    \"No Dimensionality Reduction\": {\n",
    "        \"method\": None,\n",
    "        \"n_clusters\": best_no_reduction_n_components,\n",
    "        \"covariance_type\": best_no_reduction_covariance_type\n",
    "    },\n",
    "    \"PCA\": {\n",
    "        \"method\": \"PCA\",\n",
    "        \"n_clusters\": best_pca_result['n_components'],\n",
    "        \"covariance_type\": best_pca_result['covariance_type'],\n",
    "        \"pca_threshold\": best_pca_result['pca_threshold']\n",
    "    },\n",
    "    \"t-SNE\": {\n",
    "        \"method\": \"TSNE\",\n",
    "        \"n_clusters\": best_tsne_result['n_components'],\n",
    "        \"covariance_type\": best_tsne_result['covariance_type'],\n",
    "        \"tsne_perplexity\": best_tsne_result['tsne_perplexity']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Collect metrics for each method\n",
    "methods = list(method_configs.keys())\n",
    "metrics_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "metrics_list = []\n",
    "\n",
    "for method_name, config in method_configs.items():\n",
    "    print(f\"\\nRunning {method_name}\")\n",
    "    metrics = run_dimensionality_reduction_and_cv(\n",
    "        X_scaled, y,\n",
    "        method=config.get(\"method\"),\n",
    "        n_clusters=config.get(\"n_clusters\"),\n",
    "        covariance_type=config.get(\"covariance_type\"),\n",
    "        pca_threshold=config.get(\"pca_threshold\"),\n",
    "        tsne_perplexity=config.get(\"tsne_perplexity\")\n",
    "    )\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "# Convert metrics to a NumPy array for visualization\n",
    "scores = np.array(metrics_list)\n",
    "\n",
    "# Visualization\n",
    "x = np.arange(len(methods))\n",
    "width = 0.2  \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric_label in enumerate(metrics_labels):\n",
    "    plt.bar(x + i * width, scores[:, i], width, label=metric_label)\n",
    "\n",
    "plt.title(\"Comparison of Metrics Across Dimensionality Reduction Techniques\")\n",
    "plt.ylabel(\"Metric Values\")\n",
    "plt.xlabel(\"Dimensionality Reduction Technique\")\n",
    "plt.xticks(x + width * 1.5, methods)\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Reduction and PCA demonstrate a very high precision value but a low recall and F-1 score. This is because the model seems to only be guessing positive when it is extremely sure, leading to a high precision, but this high standard causes the model to overlook other positive cases, which leads to the low recall and F1. t-SNE seems to improve the all metrics because it makes a wider range of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block evaluates the test set after it has been trained on models using data altered by either PCA, t-SNE, or no dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# evaluate test sets and store results\n",
    "methods = [\"No Dimensionality Reduction\", \"PCA\", \"t-SNE\"]\n",
    "test_conf_matrices = {}\n",
    "reduced_test_sets = {}\n",
    "test_clusters_results = {}\n",
    "\n",
    "for method in methods:\n",
    "    if method == \"No Dimensionality Reduction\":\n",
    "        cm, _, _ = evaluate_on_test_set(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            method=None\n",
    "        )\n",
    "    elif method == \"PCA\":\n",
    "        cm, X_test_reduced, test_clusters = evaluate_on_test_set(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            method=\"PCA\",\n",
    "            pca_threshold=best_pca_threshold,\n",
    "            pca_clusters=best_pca_clusters\n",
    "        )\n",
    "        reduced_test_sets[method] = X_test_reduced\n",
    "        test_clusters_results[method] = test_clusters\n",
    "    elif method == \"t-SNE\":\n",
    "        cm, X_test_reduced, test_clusters = evaluate_on_test_set(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            method=\"t-SNE\",\n",
    "            tsne_perplexity=best_perplexity,\n",
    "            tsne_clusters=best_tsne_clusters\n",
    "        )\n",
    "        reduced_test_sets[method] = X_test_reduced\n",
    "        test_clusters_results[method] = test_clusters\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    test_conf_matrices[method] = cm  # store confusion matrices\n",
    "\n",
    "\n",
    "# generate heatmaps for the confusion matrices\n",
    "for method, cm in test_conf_matrices.items():\n",
    "    plot_conf_matrix(cm, title=f\"Confusion Matrix - {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensional Reduction Analysis\n",
    "The confusion matrices for each of the reduction techniques provides valuable insight into how dimension reduction affects performance. Without any reduction, the model achieves a high number of true positives and true negatives, it also has a high amount of false positives meaning the model is mistakenly identifying non-fraud cases as fraudulent. The PCA reduced dataset performs drastically different. It predicts non-fraud almost everytime, so while it has a high rate of identifying true negatives, it also predicts an enourmous amount of false negatives. This suggests that because PCA is a linear dimensionality reduction technique, it can't capture the complex, non-linear patterns necessary to identify fraud. t-SNE also offers a much different set of performance metrics, this one being more balanced. While being able to predict all 4 metrics more consistently can lead to some positive real world effects, it still falls behind the others when it comes to actually determining fraud from non fraud which is the ultimate goal of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "The following block graphs the clusters in unique colors, with a seperate hollow red triange symbol that highlights the fraud causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot clusters dynamically for any number of clusters\n",
    "def plot_clusters(X_test_transformed, y_test, clusters, method_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # define a distinct color palette for clusters (excluding red)\n",
    "    cluster_colors = ['blue', 'green', 'orange', 'purple', 'yellow', 'cyan', 'magenta', 'brown', 'pink', 'gray']\n",
    "    unique_clusters = np.unique(clusters)\n",
    "\n",
    "    if len(unique_clusters) > len(cluster_colors):\n",
    "        raise ValueError(\"Too many clusters for the predefined color palette. Add more colors if needed.\")\n",
    "\n",
    "    # plot each cluster with a distinct color\n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        cluster_indices = (clusters == cluster)\n",
    "        plt.scatter(\n",
    "            X_test_transformed[cluster_indices, 0],\n",
    "            X_test_transformed[cluster_indices, 1],\n",
    "            c=cluster_colors[i],\n",
    "            marker='s',\n",
    "            label=f\"Cluster {cluster}\",\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "    # overlay true fraud cases (y_test == 1) as hollow red triangles\n",
    "    true_fraud_indices = (y_test == 1).values\n",
    "    plt.scatter(\n",
    "        X_test_transformed[true_fraud_indices, 0],\n",
    "        X_test_transformed[true_fraud_indices, 1],\n",
    "        edgecolor='red',\n",
    "        facecolor=\"none\",\n",
    "        marker='^',\n",
    "        linewidths=0.8,\n",
    "        s=80,\n",
    "        label=\"True Fraud (Hollow Triangle)\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Cluster Visualization - {method_name}\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(title=\"Legend\", loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# graph clusters for PCA and t-SNE\n",
    "for method in [\"PCA\", \"t-SNE\"]:\n",
    "    if method in reduced_test_sets:\n",
    "        plot_clusters(\n",
    "            reduced_test_sets[method],\n",
    "            y_test,\n",
    "            test_clusters_results[method],\n",
    "            method_name=method\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are visualized on each of the graphs, one with a PCA reduction and the other with t-SNE. In each graph, the fraudulunt data points (red hollow triangles) don't seem to correlate with any of the other clusters, instead they are distributed almost randomly throughout the existing clusters. This fact, combined with the lack of reliable testing results after many forms of preprocessing leads me to believe that the GMM model is not well suited for this classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
