{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is needed to ensure my working directory is the same as where the dataset is\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('C:\\\\Users\\\\wyman\\\\Desktop\\\\701IntroProject\\\\INM701')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Introduction\n",
    "\n",
    "The dataset is 1,000,000 rows but a sample size of 10000 is used to save time. However, it is stratified to maintain the same fraud ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE, trustworthiness\n",
    "\n",
    "df = pd.read_csv('data/card_transdata.csv')\n",
    "\n",
    "# Get counts of fraud and non-fraud cases\n",
    "fraud_count = df['fraud'].value_counts()\n",
    "min_count = fraud_count.min()\n",
    "\n",
    "# Separate fraud and non-fraud cases\n",
    "fraud_df = df[df['fraud'] == 1].sample(n=5000, random_state=42)\n",
    "non_fraud_df = df[df['fraud'] == 0].sample(n=5000, random_state=42)\n",
    "\n",
    "# Combine the balanced datasets\n",
    "df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle the final dataframe\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "This block normalizes the numeric features using StandardScaler to ensure that the Random Forest algorithm performs effectively. StandardScaler normalizes the data such that the Mean equals 0 and the Standard Deviation equals 1. The binary columns are left unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and labels\n",
    "X = df.drop(columns=['fraud'], axis=1)  \n",
    "y = df['fraud'] \n",
    "\n",
    "# separate binary columns\n",
    "binary_columns = [col for col in X.columns if set(X[col].unique()) <= {0, 1}]\n",
    "\n",
    "# separate continuous columns\n",
    "continuous_columns = [col for col in X.columns if col not in binary_columns]\n",
    "\n",
    "# scale only the continuous columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled_continuous = scaler.fit_transform(df[continuous_columns])\n",
    "\n",
    "# combine binary and scaled continuous data\n",
    "X_scaled = pd.concat(\n",
    "    [pd.DataFrame(X_scaled_continuous, columns=continuous_columns), X[binary_columns].reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check that the classification labels were split properly\n",
    "print(\"\\nValue Counts for Target Labels:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Fraud ratio: {y.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, variance_threshold): # apply PCA dimension reduction to a given dataset\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca\n",
    "\n",
    "def apply_tsne(X, perplexity, learning_rate=200): # apply t-SNE dimension reduction to a given dataset\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, learning_rate=learning_rate)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    return X_tsne, tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest_cv(X, y, n_estimators, max_depth, cv=5, scoring='accuracy'):\n",
    "    # ensure scoring is always a list in case there is only one criteria\n",
    "    if isinstance(scoring, str):\n",
    "        scoring = [scoring]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    results = cross_validate(rf, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    mean_scores = {metric: np.mean(results[f'test_{metric}']) for metric in scoring}\n",
    "\n",
    "    # return a dictionary of mean scores, even if there's only one metric\n",
    "    return mean_scores\n",
    "\n",
    "def grid_search_random_forest(X, y, n_estimators_range, max_depth_range, method_name=\"\", scoring='accuracy', verbose=True):\n",
    "    results = []\n",
    "    # loop through all combinations of hyperparameters\n",
    "    for n_estimators in n_estimators_range:\n",
    "        for depth in max_depth_range:\n",
    "            # calculate score for specific combination\n",
    "            score = run_random_forest_cv(X, y, n_estimators, depth, cv=5, scoring=scoring)\n",
    "            \n",
    "            # in case there are multiple scores from the rf cv, display the first one\n",
    "            displayed_score = score if isinstance(score, float) else next(iter(score.values()))\n",
    "\n",
    "            results.append((method_name, n_estimators, depth, displayed_score))\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"{method_name} - n_estimators: {n_estimators}, max_depth: {depth}, \"\n",
    "                    f\"Mean: {displayed_score:.4f}\"\n",
    "                )\n",
    "    return results\n",
    "\n",
    "def run_dimensionality_reduction_and_cv(X, y, method=None, n_splits=5,\n",
    "                                        n_estimators=None, max_depth=None,\n",
    "                                        pca_threshold=None, tsne_perplexity=None):\n",
    "    # apply corresponding dimension reduction\n",
    "    if method == \"PCA\":\n",
    "        print(\"Applying PCA\")\n",
    "        X_reduced, _ = apply_pca(X, pca_threshold)\n",
    "    elif method == \"TSNE\":\n",
    "        print(\"Applying t-SNE\")\n",
    "        X_reduced, _ = apply_tsne(X, tsne_perplexity)\n",
    "    elif method is None:\n",
    "        print(\"No dimensionality reduction\")\n",
    "        X_reduced = X\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'PCA', 'TSNE', or None\")\n",
    "\n",
    "    X_reduced = np.array(X_reduced)\n",
    "    y_np = np.array(y)\n",
    "\n",
    "    # perform a stratifiedkfold for cv to split the data into validation sets while maintaining the original class distribution\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    print(f\"\\nRunning Cross-Validation ({'No Dimensionality Reduction' if method is None else method})\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_reduced, y)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        X_train, X_val = X_reduced[train_idx], X_reduced[val_idx]\n",
    "        y_train, y_val = y_np[train_idx], y_np[val_idx]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_val)\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        recall = recall_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        fold_results.append((accuracy, precision, recall, f1))\n",
    "\n",
    "        print(f\"Fold Metrics - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    avg_metrics = np.mean(fold_results, axis=0)\n",
    "    print(f\"\\nAverage Metrics Across Folds: Accuracy: {avg_metrics[0]:.4f}, Precision: {avg_metrics[1]:.4f}, Recall: {avg_metrics[2]:.4f}, F1-Score: {avg_metrics[3]:.4f}\")\n",
    "    return avg_metrics\n",
    "\n",
    "def evaluate_on_test_set(X_train, y_train, X_test, y_test, method=None,\n",
    "                         n_estimators=None, max_depth=None,\n",
    "                         pca_threshold=None, tsne_perplexity=None):\n",
    "    # apply the approriate dimension reduction if necessary\n",
    "    if method == \"PCA\":\n",
    "        X_train, pca_obj = apply_pca(X_train, pca_threshold)\n",
    "        X_test = pca_obj.transform(X_test)\n",
    "    elif method == \"t-SNE\":\n",
    "        X_train, _ = apply_tsne(X_train, tsne_perplexity)\n",
    "        X_test, _ = apply_tsne(X_test, tsne_perplexity)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return cm, X_test, y_pred, rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_performance(x_values, y_values, title, xlabel, ylabel, labels=None):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    if labels is None:\n",
    "        labels = [None]*len(y_values)\n",
    "    for y, lbl in zip(y_values, labels):\n",
    "        plt.plot(x_values, y, marker='o', label=lbl)\n",
    "    if any(labels):\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Function to plot hyperparameter performance\n",
    "def plot_hyperparameter_performance(results, n_estimators_range, max_depth_range, method_name):\n",
    "    # Calculate average accuracy for each number of estimators\n",
    "    average_performance_estimators = {}\n",
    "    for n_est in n_estimators_range:\n",
    "        accuracies = [result[3] for result in results if result[1] == n_est]\n",
    "        average_performance_estimators[n_est] = np.mean(accuracies) if accuracies else 0\n",
    "    \n",
    "    # Calculate average accuracy for each max depth\n",
    "    average_performance_depth = {}\n",
    "    for depth in max_depth_range:\n",
    "        accuracies = [result[3] for result in results if result[2] == depth]\n",
    "        average_performance_depth[depth] = np.mean(accuracies) if accuracies else 0\n",
    "    \n",
    "    # Plot Average Accuracy vs Number of Estimators\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(list(average_performance_estimators.keys()), list(average_performance_estimators.values()), marker=\"o\")\n",
    "    plt.title(f\"Average Accuracy vs Number of Estimators ({method_name})\")\n",
    "    plt.xlabel(\"Number of Estimators\")\n",
    "    plt.ylabel(\"Average Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Average Accuracy vs Max Depth\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    depth_keys = [d if d is not None else 0 for d in average_performance_depth.keys()]\n",
    "    plt.plot(depth_keys, list(average_performance_depth.values()), marker=\"o\")\n",
    "    plt.title(f\"Average Accuracy vs Max Depth ({method_name})\")\n",
    "    plt.xlabel(\"Max Depth (None shown as 0)\")\n",
    "    plt.ylabel(\"Average Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_conf_matrix(cm, class_names=[\"Non-Fraud\",\"Fraud\"], title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Optimization\n",
    "PCA is run with various combinations of retained variance thresholds and hyperparameters to determine the optimal set of parameters for Random Forest with a PCA approach based primarily on the accuracy, which measures the proportion of correct predictions (both positive and negative) the model makes out of all predictions. The best combination of parameters is maintained for later testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m removed_components \u001b[38;5;241m=\u001b[39m original_components \u001b[38;5;241m-\u001b[39m retained_components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# perform grid search with PCA\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m pca_search \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_random_forest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_estimators_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPCA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# extract and store the results from each entrys\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m pca_search:\n",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m, in \u001b[0;36mgrid_search_random_forest\u001b[1;34m(X, y, n_estimators_range, max_depth_range, method_name, scoring, verbose)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_estimators \u001b[38;5;129;01min\u001b[39;00m n_estimators_range:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m max_depth_range:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# calculate score for specific combination\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_random_forest_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# in case there are multiple scores from the rf cv, display the first one\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         displayed_score \u001b[38;5;241m=\u001b[39m score \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(score\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m, in \u001b[0;36mrun_random_forest_cv\u001b[1;34m(X, y, n_estimators, max_depth, cv, scoring)\u001b[0m\n\u001b[0;32m      4\u001b[0m     scoring \u001b[38;5;241m=\u001b[39m [scoring]\n\u001b[0;32m      6\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, max_depth\u001b[38;5;241m=\u001b[39mmax_depth, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m {metric: np\u001b[38;5;241m.\u001b[39mmean(results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m scoring}\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# return a dictionary of mean scores, even if there's only one metric\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PCA thresholds for testing\n",
    "pca_thresholds = [0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "\n",
    "# hyperparameters for random forest\n",
    "n_estimators_range = [50, 100, 150]\n",
    "max_depth_range = [None, 10, 20]\n",
    "\n",
    "# to store (pca_threshold, n_estimators, max_depth, mean_accuracy)\n",
    "pca_results = []  \n",
    "\n",
    "original_components = X_scaled.shape[1]\n",
    "\n",
    "for pca_threshold in pca_thresholds:\n",
    "    X_pca, pca_obj = apply_pca(X_scaled, pca_threshold)\n",
    "    # list to keep track of which components are retained or removed\n",
    "    retained_components = pca_obj.n_components_\n",
    "    removed_components = original_components - retained_components\n",
    "\n",
    "    # perform grid search with PCA\n",
    "    pca_search = grid_search_random_forest(\n",
    "        X_pca, y, \n",
    "        n_estimators_range, max_depth_range, \n",
    "        method_name=\"PCA\", verbose=False\n",
    "    )\n",
    "    \n",
    "    # extract and store the results from each entrys\n",
    "    for entry in pca_search:\n",
    "        method, n_est, depth, acc = entry\n",
    "        pca_results.append((pca_threshold, n_est, depth, acc))\n",
    "\n",
    "    # print out metrics for each combination of PCA threshold and hyperparameters\n",
    "    for (threshold, n_est, depth, acc) in [res for res in pca_results if res[0] == pca_threshold]:\n",
    "        print(f\"PCA Threshold: {threshold}, n_estimators: {n_est}, max_depth: {depth}, Mean Accuracy: {acc:.4f}, \"\n",
    "              f\"Retained: {retained_components}, Removed: {removed_components}\")\n",
    "\n",
    "# find the best PCA values based on accuracy, and its corresponding hyperparameters\n",
    "best_pca_trial = max(pca_results, key=lambda x: x[3])\n",
    "best_pca_threshold, best_n_estimators_pca, best_max_depth_pca, best_accuracy_pca = best_pca_trial\n",
    "\n",
    "print(f\"\\nBest PCA Threshold: {best_pca_threshold}, Best n_estimators: {best_n_estimators_pca}, \"\n",
    "      f\"Best max_depth: {best_max_depth_pca}, Best Mean Accuracy: {best_accuracy_pca:.4f}\")\n",
    "\n",
    "results_array = np.array(pca_results, dtype=object)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for n_estimators in n_estimators_range:\n",
    "    for max_depth in max_depth_range:\n",
    "        indices = (results_array[:, 1] == n_estimators) & (results_array[:, 2] == max_depth)\n",
    "        thresholds = results_array[indices][:, 0]\n",
    "        accuracies = results_array[indices][:, 3].astype(float)\n",
    "        plt.plot(thresholds, accuracies, marker='o', label=f'n_estimators={n_estimators}, max_depth={max_depth}')\n",
    "\n",
    "plt.title(\"Mean Accuracy for PCA Thresholds and Random Forest Hyperparameters\")\n",
    "plt.xlabel(\"PCA Threshold\")\n",
    "plt.ylabel(\"Mean Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of estimators or depth did not seem to have a real effect on the mean accuracy compared to increasing the PCA threshold. The base accuracy is already high so applying the model to the PCA reduced dataset seems very well-suited for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Optimization\n",
    "t-SNE is run with various combinations of perplexities and hyperparameters to determine the optimal set of parameters for Random Forest with a t-SNE approach based primarily on the silhouette score and the ARI. The trustworthiness is useful to verify that the local structure of the data is properly reflected after the dimension reduction, which is important for visualization, but the other metrics better measure how well the data is preservered after being reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE perplexities and RF hyperparameters\n",
    "tsne_perplexities = [5, 10, 20, 30, 40]\n",
    "n_estimators_range = [50, 100, 150]\n",
    "max_depth_range = [None, 10, 20]\n",
    "\n",
    "# store results as (perplexity, n_estimators, max_depth, mean_accuracy, trustworthiness)\n",
    "tsne_results = []\n",
    "\n",
    "for perplexity in tsne_perplexities:\n",
    "    # apply t-SNE to transform the dataset\n",
    "    X_tsne, tsne_obj = apply_tsne(X_scaled, perplexity)\n",
    "    \n",
    "    trust = trustworthiness(X_scaled, X_tsne, n_neighbors=5)\n",
    "    \n",
    "    # perform grid search with tsne reduced dataset\n",
    "    tsne_search_results = grid_search_random_forest(\n",
    "        X_tsne, y,\n",
    "        n_estimators_range, max_depth_range,\n",
    "        method_name=\"t-SNE (perplexity={})\".format(perplexity),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # extract and store the results from each entry\n",
    "    for entry in tsne_search_results:\n",
    "        method, n_est, depth, acc = entry\n",
    "        tsne_results.append((perplexity, n_est, depth, acc, trust))\n",
    "\n",
    "    # print out results for the current perplexity\n",
    "    for (perplexity, n_est, depth, acc, tw) in [res for res in tsne_results if res[0] == perplexity]:\n",
    "        print(f\"t-SNE Perplexity: {perplexity}, n_estimators: {n_est}, max_depth: {depth}, Mean Accuracy: {acc:.4f}, Trustworthiness: {tw:.4f}\")\n",
    "\n",
    "# find the best combination based on accuracy\n",
    "best_tsne_trial = max(tsne_results, key=lambda x: x[3])  # maximize accuracy\n",
    "best_tsne_perplexity, best_n_estimators_tsne, best_max_depth_tsne, best_accuracy_tsne, best_trust_tsne = best_tsne_trial\n",
    "\n",
    "print(f\"\\nBest t-SNE Perplexity: {best_tsne_perplexity}, Best n_estimators: {best_n_estimators_tsne}, \"\n",
    "      f\"Best max_depth: {best_max_depth_tsne}, Best Mean Accuracy: {best_accuracy_tsne:.4f}, \"\n",
    "      f\"Best Trustworthiness: {best_trust_tsne:.4f}\")\n",
    "\n",
    "tsne_results_array = np.array(tsne_results, dtype=object)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for n_estimators in n_estimators_range:\n",
    "    for max_depth in max_depth_range:\n",
    "        indices = (tsne_results_array[:, 1] == n_estimators) & (tsne_results_array[:, 2] == max_depth)\n",
    "        perplexities = tsne_results_array[indices][:, 0]\n",
    "        accuracies = tsne_results_array[indices][:, 3].astype(float)\n",
    "        plt.plot(perplexities, accuracies, marker='o', label=f'n_estimators={n_estimators}, max_depth={max_depth}')\n",
    "\n",
    "plt.title(\"Mean Accuracy for t-SNE Perplexities and Random Forest Hyperparameters\")\n",
    "plt.xlabel(\"t-SNE Perplexity\")\n",
    "plt.ylabel(\"Mean Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, accuracy seems to slightly decrease as you increase the t-SNE perplexities. This could be because higher perplexities causes t-SNE to focus on larger structures of data, which can smooth out differences between points and lead to a loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Hyperparameter Refinement\n",
    "This block runs another grid search to find the optimal depth and amount of trees in the forest based on the optimal PCA and t-SNE parameters previously calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\nRefining hyperparameters for PCA, t-SNE, and No Reduction\")\n",
    "\n",
    "# Define broader hyperparameter ranges\n",
    "n_estimators_range = [50, 100, 150, 200, 250]\n",
    "max_depth_range = [None, 10, 20, 30]\n",
    "\n",
    "# Refine for No Dimensionality Reduction\n",
    "print(\"\\nTuning hyperparameters for No Reduction...\")\n",
    "no_reduction_results = grid_search_random_forest(\n",
    "    X_scaled, y, n_estimators_range, max_depth_range, \"No Reduction\"\n",
    ")\n",
    "\n",
    "# Find the best hyperparameters for No Reduction\n",
    "best_refined_no_reduction = max(no_reduction_results, key=lambda x: x[3])  # Assuming x[3] is accuracy\n",
    "best_n_estimators_no_reduction, best_max_depth_no_reduction = best_refined_no_reduction[1:3]\n",
    "\n",
    "print(f\"Refined Best No Reduction - n_estimators: {best_n_estimators_no_reduction}, \"\n",
    "      f\"max_depth: {best_max_depth_no_reduction}\")\n",
    "\n",
    "# Plot for No Reduction\n",
    "plot_hyperparameter_performance(no_reduction_results, n_estimators_range, max_depth_range, \"No Reduction\")\n",
    "\n",
    "# Refine for PCA\n",
    "print(\"\\nTuning hyperparameters for PCA...\")\n",
    "X_pca_best, _ = apply_pca(X_scaled, best_pca_threshold)\n",
    "pca_refined_results = grid_search_random_forest(\n",
    "    X_pca_best, y, n_estimators_range, max_depth_range, \"PCA\"\n",
    ")\n",
    "\n",
    "# Find the best hyperparameters for PCA\n",
    "best_refined_pca = max(pca_refined_results, key=lambda x: x[3])\n",
    "best_n_estimators_pca_refined, best_max_depth_pca_refined = best_refined_pca[1:3]\n",
    "\n",
    "print(f\"Refined Best PCA - n_estimators: {best_n_estimators_pca_refined}, \"\n",
    "      f\"max_depth: {best_max_depth_pca_refined}\")\n",
    "\n",
    "# Plot for PCA\n",
    "plot_hyperparameter_performance(pca_refined_results, n_estimators_range, max_depth_range, \"PCA\")\n",
    "\n",
    "# Refine for t-SNE\n",
    "print(\"\\nTuning hyperparameters for t-SNE...\")\n",
    "X_tsne_best, _ = apply_tsne(X_scaled, best_tsne_perplexity)\n",
    "tsne_refined_results = grid_search_random_forest(\n",
    "    X_tsne_best, y, n_estimators_range, max_depth_range, \"t-SNE\"\n",
    ")\n",
    "\n",
    "# Find the best hyperparameters for t-SNE\n",
    "best_refined_tsne = max(tsne_refined_results, key=lambda x: x[3])\n",
    "best_n_estimators_tsne_refined, best_max_depth_tsne_refined = best_refined_tsne[1:3]\n",
    "\n",
    "print(f\"Refined Best t-SNE - n_estimators: {best_n_estimators_tsne_refined}, \"\n",
    "      f\"max_depth: {best_max_depth_tsne_refined}\")\n",
    "\n",
    "# Plot for t-SNE\n",
    "plot_hyperparameter_performance(tsne_refined_results, n_estimators_range, max_depth_range, \"t-SNE\")\n",
    "\n",
    "# Summary of Best Hyperparameters\n",
    "print(\"\\nSummary of Best Hyperparameters for Each Method:\")\n",
    "print(f\"No Reduction:\")\n",
    "print(f\"  Number of Estimators: {best_n_estimators_no_reduction}\")\n",
    "print(f\"  Max Depth: {best_max_depth_no_reduction}\")\n",
    "\n",
    "print(f\"PCA:\")\n",
    "print(f\"  Number of Estimators: {best_n_estimators_pca_refined}\")\n",
    "print(f\"  Max Depth: {best_max_depth_pca_refined}\")\n",
    "\n",
    "print(f\"t-SNE:\")\n",
    "print(f\"  Number of Estimators: {best_n_estimators_tsne_refined}\")\n",
    "print(f\"  Max Depth: {best_max_depth_tsne_refined}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining the optimal PCA retained variance and t-SNE complexity, a new grid search is conducted to find the optimal random forest hyperparameters. Increasing the amount of trees in each layer relatively increases accuracy, but it was already extremely high to begin with. The max depth has a small dip at the threshold value of 10 but once it is crossed the accuracy remains at a consistent high point, although it is always extremely high either way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "This block runs cross validation using either PCA, t-SNE, or no dimension reduction and compares the metrics of the outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for no dimensionality reduction\n",
    "metrics_no_reduction = run_dimensionality_reduction_and_cv(\n",
    "    X_scaled, y, method=None,\n",
    "    n_estimators=best_n_estimators_no_reduction, max_depth=best_max_depth_no_reduction\n",
    ")\n",
    "\n",
    "# run for PCA\n",
    "metrics_pca = run_dimensionality_reduction_and_cv(\n",
    "    X_scaled, y, method=\"PCA\",\n",
    "    n_estimators=best_n_estimators_pca, max_depth=best_max_depth_pca, pca_threshold=best_pca_threshold\n",
    ")\n",
    "\n",
    "# run for t-SNE\n",
    "metrics_tsne = run_dimensionality_reduction_and_cv(\n",
    "    X_scaled, y, method=\"TSNE\",\n",
    "    n_estimators=best_n_estimators_tsne, max_depth=best_max_depth_tsne, tsne_perplexity=best_tsne_perplexity\n",
    ")\n",
    "\n",
    "# collect metrics for plotting\n",
    "methods = [\"No Dimensionality Reduction\", \"PCA\", \"t-SNE\"]\n",
    "metrics_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "# create list to store metrics\n",
    "metrics_list = [metrics_no_reduction, metrics_pca, metrics_tsne]\n",
    "\n",
    "# convert metrics_list to a NumPy array for easy indexing\n",
    "scores = np.array(metrics_list)\n",
    "\n",
    "# define data and labels\n",
    "x = np.arange(len(methods))\n",
    "width = 0.2  \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric_label in enumerate(metrics_labels):\n",
    "    plt.bar(x + i * width, scores[:, i], width, label=metric_label)\n",
    "\n",
    "plt.title(\"Comparison of Metrics Across Dimensionality Reduction Techniques\")\n",
    "plt.ylabel(\"Metric Values\")\n",
    "plt.xlabel(\"Dimensionality Reduction Technique\")\n",
    "plt.xticks(x + width * 1.5, methods)\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a CV that compares the different reduction techniques, the model performs extremely well regardless of which approach is chosen. However, the unreduced dataset performs slightly better than the datasets with their dimensions reduced. This might be because the reduction is removing important information and the model is able to handle the extra noise well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Evaluation\n",
    "The following block evaluates the test set after it has been trained on models using data altered by either PCA, t-SNE, or no dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# evaluate No Reduction and plot the confusion matrix\n",
    "cm_no_red, _, _, rf_no_red = evaluate_on_test_set(\n",
    "    X_train, y_train, X_test, y_test, method=None,\n",
    "    n_estimators=best_n_estimators_no_reduction, max_depth=best_max_depth_no_reduction\n",
    ")\n",
    "plot_conf_matrix(cm_no_red, title=\"Confusion Matrix - No Reduction\")\n",
    "\n",
    "# evaluate PCA and plot the confusion matrix\n",
    "cm_pca, X_test_pca, y_pred_pca, rf_pca = evaluate_on_test_set(\n",
    "    X_train, y_train, X_test, y_test, method=\"PCA\",\n",
    "    n_estimators=best_n_estimators_pca_refined, max_depth=best_max_depth_pca_refined,\n",
    "    pca_threshold=best_pca_threshold\n",
    ")\n",
    "plot_conf_matrix(cm_pca, title=\"Confusion Matrix - PCA\")\n",
    "\n",
    "# evaluate t-SNE and plot the confusion matrix\n",
    "cm_tsne, X_test_tsne, y_pred_tsne, rf_tsne = evaluate_on_test_set(\n",
    "    X_train, y_train, X_test, y_test, method=\"t-SNE\",\n",
    "    n_estimators=best_n_estimators_tsne_refined, max_depth=best_max_depth_tsne_refined,\n",
    "    tsne_perplexity=best_tsne_perplexity\n",
    ")\n",
    "plot_conf_matrix(cm_tsne, title=\"Confusion Matrix - t-SNE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset with no dimensionality reduction performed slightly better than the PCA reduced dataset, but both of these datasets were very reliable and had high rates of true positives and true negatives. Dimension reduction with t-SNE offered a substantial decrease in performance, identifiying many more false positives and false negatives. t-SNE is more focused on data visualization rather than improving the rate of classification. One problem with t-SNE is that it always reduces to 2 dimensions which could lead to a large loss of relevent information and a less accurate model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
